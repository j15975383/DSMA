{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSMAv3 - Goemotions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/cT5VPqsA8EpfkcM/MX/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j15975383/DSMA/blob/main/DSMAv3_Goemotions_Multilabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/monologg/GoEmotions-pytorch.git\n",
        "!gdown --id \"1j_bwko_aAYGs5W0wqY7cvIFqPIkNcQv7\" --output \"test_set_v2.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RWK7S3FIU5t",
        "outputId": "aaefc2ea-46d2-45d3-e3b7-469ded28f33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GoEmotions-pytorch' already exists and is not an empty directory.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j_bwko_aAYGs5W0wqY7cvIFqPIkNcQv7\n",
            "To: /content/test_set_v2.csv\n",
            "100% 70.0k/70.0k [00:00<00:00, 29.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers #--force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KsWnNoN6RWa",
        "outputId": "cc35cf7a-a205-4a38-d7c2-d8550baa7915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizer\n",
        "\n",
        "# specify GPU\n",
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2ge2mgh76Sw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1he9y_T6UYH",
        "outputId": "81cbdf3d-f5af-4706-dd94-05dbc871a4bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_set = pd.read_csv(\"test_set_v2.csv\")\n",
        "test_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "85NVfHUHIux7",
        "outputId": "d61ea64d-6855-48a1-8cb1-4e9f4b56dd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a00699ad-9ab3-495b-8834-8d9f9528e7c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Xbox one X player along with series X player ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>&gt;Yep all they had to do was just reskin BF5 wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Or 6 Jeep’s</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I did and four out of my five friends who play...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Dont know about this argument because IMO ever...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>195</td>\n",
              "      <td>Me too, and now I’m sad.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>I liked it but it wasn't quite bf. I hope this...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>This might be a stupid question. (I played may...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>They actually removed suppression lmao what th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>We don’t want to move on though. We love Battl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a00699ad-9ab3-495b-8834-8d9f9528e7c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a00699ad-9ab3-495b-8834-8d9f9528e7c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a00699ad-9ab3-495b-8834-8d9f9528e7c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Unnamed: 0                                               text  label\n",
              "0             0   Xbox one X player along with series X player ...      1\n",
              "1             1  >Yep all they had to do was just reskin BF5 wi...      1\n",
              "2             2                                        Or 6 Jeep’s      0\n",
              "3             3  I did and four out of my five friends who play...      1\n",
              "4             4  Dont know about this argument because IMO ever...      2\n",
              "..          ...                                                ...    ...\n",
              "195         195                           Me too, and now I’m sad.      1\n",
              "196         196  I liked it but it wasn't quite bf. I hope this...      1\n",
              "197         197  This might be a stupid question. (I played may...      2\n",
              "198         198  They actually removed suppression lmao what th...      1\n",
              "199         199  We don’t want to move on though. We love Battl...      1\n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/GoEmotions-pytorch/data/group/train.tsv\",sep = '\\t', header = None)\n",
        "labels = pd.read_csv(\"/content/GoEmotions-pytorch/data/group/labels.txt\")\n",
        "test = pd.read_csv(\"/content/GoEmotions-pytorch/data/group/test.tsv\",sep = '\\t',header = None)"
      ],
      "metadata": {
        "id": "3VO9vt4m6YOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.info())\n",
        "print(train.head())\n",
        "train[1].value_counts(normalize = True)\n",
        "#Here we found out that there are cases where there are multilabels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR5QDy_I6f3n",
        "outputId": "e52f01d5-d68b-4fb2-98e8-4b7c8b7a9251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 43410 entries, 0 to 43409\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       43410 non-null  object\n",
            " 1   1       43410 non-null  object\n",
            " 2   2       43410 non-null  object\n",
            "dtypes: object(3)\n",
            "memory usage: 1017.5+ KB\n",
            "None\n",
            "                                                   0  1        2\n",
            "0  My favourite food is anything I didn't have to...  2  eebbqej\n",
            "1  Now if he does off himself, everyone will thin...  2  ed00q6i\n",
            "2                     WHY THE FUCK IS BAYLESS ISOING  1  eezlygj\n",
            "3                        To make her feel threatened  1  ed7ypvh\n",
            "4                             Dirty Southern Wankers  1  ed0bdzj\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3        0.350518\n",
              "2        0.295393\n",
              "1        0.187353\n",
              "0        0.088874\n",
              "1,3      0.018083\n",
              "0,3      0.017853\n",
              "2,3      0.013660\n",
              "1,2      0.011126\n",
              "0,1      0.009145\n",
              "0,2      0.006957\n",
              "0,1,3    0.000622\n",
              "1,2,3    0.000230\n",
              "0,1,2    0.000092\n",
              "0,2,3    0.000092\n",
              "Name: 1, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter = train[1].str.len()>1\n",
        "train_single = train[~filter]\n",
        "train_single[1].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX7TTbzS6k4V",
        "outputId": "1886ffa2-0232-4fb9-88fb-472a4e3b0b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    0.380115\n",
              "2    0.320335\n",
              "1    0.203173\n",
              "0    0.096378\n",
              "Name: 1, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter = test[1].str.len()>1\n",
        "test_single = test[~filter]\n",
        "test_single[1].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpF2MZas8ZEu",
        "outputId": "e61673b1-9c7d-4d46-8743-2ea4615d759c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    0.370599\n",
              "2    0.319475\n",
              "1    0.212851\n",
              "0    0.097076\n",
              "Name: 1, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['label'].value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKHV8PyT75fM",
        "outputId": "710da871-dcc9-476a-c135-e4a979b997a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.380\n",
              "0    0.270\n",
              "3    0.195\n",
              "2    0.155\n",
              "Name: label, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.tensor requires the labels to be ints instead of strs\n",
        "#we hereby change the datatype of the labels\n",
        "train_single[1] = train_single[1].apply(lambda x: int(x))\n",
        "test_single[1] = test_single[1].apply(lambda x: int(x))\n",
        "print(train_single.info())\n",
        "print(test_single.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7K-RZIU8GmE",
        "outputId": "16df3da3-070d-4f20-ba02-117d292dbfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 40030 entries, 0 to 43409\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       40030 non-null  object\n",
            " 1   1       40030 non-null  int64 \n",
            " 2   2       40030 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.2+ MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5027 entries, 0 to 5426\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       5027 non-null   object\n",
            " 1   1       5027 non-null   int64 \n",
            " 2   2       5027 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 157.1+ KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert = AutoModel.from_pretrained(\"bert-base-cased\",return_dict = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mKZOknw8eLR",
        "outputId": "227ff40c-4b94-492d-b51c-a9100bc4154c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#just to test things out\n",
        "text = [\"I hate you Anakin\",\"You were like a brother to me\"]\n",
        "tokens = tokenizer.batch_encode_plus(text,padding = True, return_token_type_ids=False)\n",
        "print(tokens)\n",
        "tokens['input_ids']\n",
        "tokenizer.batch_decode(tokens['input_ids'])\n",
        "#or we can use\n",
        "#tokenizer.decode(tokens['input_ids'][0])\n",
        "#to find the decoded first sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiEYtbMh8jC_",
        "outputId": "35c1fda0-c83c-468a-ba35-8d2050ae3f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 146, 4819, 1128, 9954, 4314, 102, 0, 0], [101, 1192, 1127, 1176, 170, 1711, 1106, 1143, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] I hate you Anakin [SEP] [PAD] [PAD]',\n",
              " '[CLS] You were like a brother to me [SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in train[0]]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "#since most of them fall well within 30, we'll make the max_seq_length =30\n",
        "max_seq_len = 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "PAp25TjS8n-J",
        "outputId": "c7a4e0ec-cc9e-494a-fcc0-b4cbf6056f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split test dataset into validation and test\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(test_single[0], test_single[1], \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.5, \n",
        "                                                                stratify=test_single[1])"
      ],
      "metadata": {
        "id": "N1Sc5Iyp8qnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_single[0].values.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding = True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding = True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding = True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")"
      ],
      "metadata": {
        "id": "hiix33n08waD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for train set\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_single[1].values.tolist())\n",
        "\n",
        "# for validation set\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "# for test set\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "hjQ5nTxJ8xgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "B68wdjS38yyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "oijtBsiY8z5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "      \n",
        "      super(BERT_Arch, self).__init__()\n",
        "\n",
        "      self.bert = bert \n",
        "      \n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "\n",
        "      # dense layer 1\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      \n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,4)\n",
        "\n",
        "      #softmax activation function\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "      #pass the inputs to the model  \n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "      \n",
        "      x = self.fc1(cls_hs)\n",
        "\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      \n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "jCtTr7mq80xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "3IRd8ybZ81xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "paRQBzXH82nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "train_labels = train_single[1].values\n",
        "train_labels = pd.Series(train_labels)\n",
        "#compute the class weights\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels                                                    \n",
        "                                    )\n",
        "#class_weights = dict(zip(np.unique(train_labels), class_weights)),\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou3lAynt83dq",
        "outputId": "64e447dd-d13b-4275-8403-86ab30f65a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.5939606  1.23048076 0.7804336  0.65769585]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert class weights to tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "# loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "l-v7ptAQ84yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save model predictions\n",
        "  total_preds=[]\n",
        "  \n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    \n",
        "    # progress update after every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [r.to(device) for r in batch]\n",
        " \n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # clear previously calculated gradients \n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions for the current batch\n",
        "    preds = model(sent_id, mask)\n",
        "\n",
        "    # compute the loss between actual and predicted values\n",
        "    loss = cross_entropy(preds, labels)\n",
        "\n",
        "    # add on to the total loss\n",
        "    total_loss = total_loss + loss.item()\n",
        "\n",
        "    # backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # model predictions are stored on GPU. So, push it to CPU\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  # compute the training loss of the epoch\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  #returns the loss and predictions\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "lRX1Mmdz85rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "  \n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "\n",
        "  # iterate over batches\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    \n",
        "    # Progress update every 50 batches.\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      \n",
        "      # Calculate elapsed time in minutes.\n",
        "      #elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "      # Report progress.\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "    # push the batch to gpu\n",
        "    batch = [t.to(device) for t in batch]\n",
        "\n",
        "    sent_id, mask, labels = batch\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      preds = model(sent_id, mask)\n",
        "\n",
        "      # compute the validation loss between actual and predicted values\n",
        "      loss = cross_entropy(preds,labels)\n",
        "\n",
        "      total_loss = total_loss + loss.item()\n",
        "\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "\n",
        "      total_preds.append(preds)\n",
        "\n",
        "  # compute the validation loss of the epoch\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "vGq8zPo186o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7jFp8jL87jl",
        "outputId": "54a7344b-a2f5-457b-e2ef-11a61df72275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.188\n",
            "Validation Loss: 1.121\n",
            "\n",
            " Epoch 2 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.168\n",
            "Validation Loss: 1.175\n",
            "\n",
            " Epoch 3 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.160\n",
            "Validation Loss: 1.100\n",
            "\n",
            " Epoch 4 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.145\n",
            "Validation Loss: 1.096\n",
            "\n",
            " Epoch 5 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.139\n",
            "Validation Loss: 1.089\n",
            "\n",
            " Epoch 6 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.138\n",
            "Validation Loss: 1.089\n",
            "\n",
            " Epoch 7 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.141\n",
            "Validation Loss: 1.083\n",
            "\n",
            " Epoch 8 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.127\n",
            "Validation Loss: 1.109\n",
            "\n",
            " Epoch 9 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.128\n",
            "Validation Loss: 1.063\n",
            "\n",
            " Epoch 10 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.119\n",
            "Validation Loss: 1.089\n",
            "\n",
            " Epoch 11 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.129\n",
            "Validation Loss: 1.088\n",
            "\n",
            " Epoch 12 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.125\n",
            "Validation Loss: 1.058\n",
            "\n",
            " Epoch 13 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.115\n",
            "Validation Loss: 1.096\n",
            "\n",
            " Epoch 14 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.121\n",
            "Validation Loss: 1.046\n",
            "\n",
            " Epoch 15 / 15\n",
            "  Batch    50  of  1,251.\n",
            "  Batch   100  of  1,251.\n",
            "  Batch   150  of  1,251.\n",
            "  Batch   200  of  1,251.\n",
            "  Batch   250  of  1,251.\n",
            "  Batch   300  of  1,251.\n",
            "  Batch   350  of  1,251.\n",
            "  Batch   400  of  1,251.\n",
            "  Batch   450  of  1,251.\n",
            "  Batch   500  of  1,251.\n",
            "  Batch   550  of  1,251.\n",
            "  Batch   600  of  1,251.\n",
            "  Batch   650  of  1,251.\n",
            "  Batch   700  of  1,251.\n",
            "  Batch   750  of  1,251.\n",
            "  Batch   800  of  1,251.\n",
            "  Batch   850  of  1,251.\n",
            "  Batch   900  of  1,251.\n",
            "  Batch   950  of  1,251.\n",
            "  Batch 1,000  of  1,251.\n",
            "  Batch 1,050  of  1,251.\n",
            "  Batch 1,100  of  1,251.\n",
            "  Batch 1,150  of  1,251.\n",
            "  Batch 1,200  of  1,251.\n",
            "  Batch 1,250  of  1,251.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     79.\n",
            "\n",
            "Training Loss: 1.115\n",
            "Validation Loss: 1.059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "2-xpcTzv8-dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajQX09xf9ABj",
        "outputId": "cdd40909-483b-4e17-837a-2cf7dc6cf10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.68      0.42       244\n",
            "           1       0.50      0.53      0.51       535\n",
            "           2       0.51      0.52      0.51       803\n",
            "           3       0.83      0.51      0.63       932\n",
            "\n",
            "    accuracy                           0.53      2514\n",
            "   macro avg       0.53      0.56      0.52      2514\n",
            "weighted avg       0.60      0.53      0.55      2514\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['label'] = test_set['label'].apply(lambda x: int(x))"
      ],
      "metadata": {
        "id": "YWf2yh3J_seI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = test_set['text']\n",
        "test_labels = test_set['label']\n",
        "\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = max_seq_len,\n",
        "    padding = True,\n",
        "    truncation=True,\n",
        "    return_token_type_ids=False\n",
        ")\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "6QU3qUf69azv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "u9tYWpDE925K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO2o_tjq933N",
        "outputId": "1c0a128f-cca2-43f1-b82d-200477fb849d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.33      0.33        54\n",
            "           1       0.53      0.21      0.30        76\n",
            "           2       0.18      0.52      0.27        31\n",
            "           3       0.35      0.23      0.28        39\n",
            "\n",
            "    accuracy                           0.29       200\n",
            "   macro avg       0.35      0.32      0.29       200\n",
            "weighted avg       0.39      0.29      0.30       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}